<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Face Verification</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background: #000;
      color: #0f0;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      height: 100vh;
      margin: 0;
    }
    h1 {
      margin-bottom: 40px;
      font-size: 2.5em;
    }
    #status {
      margin-top: 20px;
      font-size: 1.5em;
      font-weight: bold;
    }
    .btn {
      background-color: #0f0;
      color: #000;
      border: none;
      padding: 25px 50px;
      font-size: 1.5em;
      border-radius: 12px;
      cursor: pointer;
      transition: transform 0.2s, background 0.2s;
    }
    .btn:hover {
      transform: scale(1.1);
      background-color: #0c0;
    }
  </style>
</head>
<body>
  <h1>Face Verification</h1>
  <button class="btn" onclick="startVerification()">Click to Verify</button>
  <div id="status">Not Verified Yet</div>

<script>
  const logEl = document.getElementById('log');
  const st = id => document.getElementById('status').innerText = id;
  function log(...args){
    console.log(...args);
    logEl.textContent += args.join(' ') + '\n';
    logEl.scrollTop = logEl.scrollHeight;
  }

  const MODELS_URI = './models';
  const REFERENCE_IMAGE = './reference.jpg';
  const THRESHOLD = 0.45; // lower = stricter match
  let referenceDescriptor = null;

  async function loadModels() {
    st('loading models...');
    await Promise.all([
      faceapi.nets.tinyFaceDetector.loadFromUri(MODELS_URI),
      faceapi.nets.faceLandmark68Net.loadFromUri(MODELS_URI),
      faceapi.nets.faceRecognitionNet.loadFromUri(MODELS_URI)
    ]);
    st('models loaded');
    log('Models loaded successfully');
  }

  async function startCamera() {
    st('starting camera...');
    const video = document.getElementById('video');
    const stream = await navigator.mediaDevices.getUserMedia({ video: true });
    video.srcObject = stream;
    return new Promise(resolve => video.onloadedmetadata = () => resolve(video));
  }

  async function loadReferenceImage() {
    st('loading reference image...');
    const img = await faceapi.fetchImage(REFERENCE_IMAGE);
    const detection = await faceapi.detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())
      .withFaceLandmarks()
      .withFaceDescriptor();
    if (!detection) throw new Error('No face found in reference image. Use a clear frontal face.');
    referenceDescriptor = detection.descriptor;
    st('reference image loaded');
    log('Reference descriptor ready');
  }

  async function verifyFace() {
    st('verifying...');
    const video = document.getElementById('video');
    const detection = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
      .withFaceLandmarks()
      .withFaceDescriptor();
    if (!detection) {
      document.getElementById('result').innerText = 'No face detected on camera. Move closer or improve lighting.';
      st('no face detected');
      log('No face detected from camera');
      return;
    }
    const distance = faceapi.euclideanDistance(referenceDescriptor, detection.descriptor);
    log('Distance:', distance);
    if (distance < THRESHOLD) {
      document.getElementById('result').innerText = `MATCH ✅ (distance ${distance.toFixed(3)})`;
      st('verified');
    } else {
      document.getElementById('result').innerText = `NO MATCH ❌ (distance ${distance.toFixed(3)})`;
      st('not matching');
    }
  }

  async function init() {
    try {
      if(location.protocol === 'file:') log('⚠️ Run this on localhost server, not file://, to avoid model load issues.');
      await loadModels();
      await startCamera();
      await loadReferenceImage();
      st('ready — press "Verify Face"');
    } catch (err) {
      log('Initialization error:', err);
      st('error during init — check log');
    }
  }

  document.getElementById('verifyBtn').onclick = verifyFace;
  window.addEventListener('load', init);
</script>
</body>
</html>
